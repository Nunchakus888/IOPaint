"""
ä¼˜åŒ–ç‰ˆæ°´å°æ£€æµ‹å™¨ - ä¸“æ³¨äºä¼ ç»Ÿæ–¹æ³•çš„ç²¾ç®€ä¼˜åŒ–
============================================

æ ¸å¿ƒç†å¿µï¼šä¼ ç»Ÿæ–¹æ³•ä¼˜åŒ–ï¼Œä¿æŠ¤åŸå›¾è´¨é‡

é—®é¢˜åˆ†æ - å½“å‰æ–¹æ³•çš„å±€é™æ€§ï¼š
1. æµç¨‹å¤æ‚ï¼šOCRæ£€æµ‹ â†’ è¿‡æ»¤ â†’ è½®å»“æå– â†’ å½¢æ€å­¦æ“ä½œ â†’ ä¸»ä½“ä¿æŠ¤ â†’ æ¡¥æ¥é—­è¿ç®—
2. è¡¥æ•‘æªæ–½å †ç§¯ï¼šä¸ºäº†è§£å†³è¯¯ä¼¤ï¼Œå¼•å…¥æ— æ•°å¯å‘å¼è§„åˆ™ï¼Œéš¾ä»¥ç»´æŠ¤
3. AIæ–¹æ³•ç ´ååŸå›¾ï¼šå¤§æ¨¡å‹æ¨ç†ä¼šæ¨¡ç³Šæ•´ä¸ªå›¾åƒï¼Œç ´ååƒç´ è´¨é‡
4. ç¼ºä¹é’ˆå¯¹æ€§ï¼šæ²¡æœ‰ä¸“é—¨ä¼˜åŒ–ä¼ ç»Ÿæ–¹æ³•çš„æ£€æµ‹æ•ˆæœ

ä¼˜åŒ–æ–¹æ¡ˆï¼šå¤šç­–ç•¥ä¼ ç»Ÿæ£€æµ‹ + è½»é‡çº§å¤„ç† + ä¿æŠ¤åŸå›¾
"""

import cv2
import numpy as np
from typing import List, Tuple, Dict, Optional, Any
from dataclasses import dataclass
import torch
import torch.nn.functional as F
from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation
import easyocr
from PIL import Image


@dataclass
class WatermarkDetection:
    """æ°´å°æ£€æµ‹ç»“æœ"""
    bbox: np.ndarray  # è¾¹ç•Œæ¡†
    confidence: float  # ç½®ä¿¡åº¦
    text: str  # è¯†åˆ«çš„æ–‡å­—
    mask: np.ndarray  # ç²¾ç¡®mask
    category: str  # åˆ†ç±»ï¼šwatermark/text/subject


class OptimizedWatermarkDetector:
    """
    ä¼˜åŒ–ç‰ˆæ°´å°æ£€æµ‹å™¨ - ä¸“æ³¨äºä¼ ç»Ÿæ–¹æ³•ä¼˜åŒ–

    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. å¤šç­–ç•¥æ£€æµ‹ï¼šå¤šç§è¾¹ç¼˜æ£€æµ‹å¹¶è¡Œï¼Œæé«˜å¬å›ç‡
    2. ä¿æŠ¤åŸå›¾ï¼šè½»é‡çº§å¤„ç†ï¼Œé¿å…åƒç´ ç ´å
    3. æ™ºèƒ½è¿‡æ»¤ï¼šåŸºäºå‡ ä½•ç‰¹å¾è¿‡æ»¤è¯¯æ£€
    4. ç²¾ç¡®maskï¼šç”Ÿæˆé«˜è´¨é‡çš„äºŒå€¼mask
    """

    def __init__(self, enable_preview: bool = True):
        # ä¸“æ³¨ä¼˜åŒ–ä¼ ç»Ÿæ–¹æ³•ï¼Œä¿æŠ¤åŸå›¾è´¨é‡
        self.enable_preview = enable_preview

        # è½»é‡çº§OCR - åªç”¨äºéªŒè¯ï¼Œä¸ç”¨äºä¸»è¦æ£€æµ‹
        try:
            self.reader = easyocr.Reader(['en', 'ch_sim'], gpu=torch.cuda.is_available())
        except:
            self.reader = None

    def detect_watermarks(self, image: np.ndarray, preview_path: Optional[str] = None) -> List[WatermarkDetection]:
        """
        ä¸»æ£€æµ‹æµç¨‹ - ä¸“æ³¨äºä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŒ–

        é˜¶æ®µ1: å¤šç­–ç•¥è¾¹ç¼˜æ£€æµ‹ - è¯†åˆ«æ½œåœ¨æ°´å°åŒºåŸŸï¼ˆä¿æŠ¤åŸå›¾ï¼‰
        é˜¶æ®µ2: ç‰¹å¾éªŒè¯ - ç¡®è®¤æ°´å°ç‰¹å¾
        é˜¶æ®µ3: ç²¾ç¡®maskç”Ÿæˆ - ç”Ÿæˆæœ€ç»ˆmask
        """
        print("ğŸš€ Starting traditional watermark detection...")

        # åˆå§‹åŒ–é¢„è§ˆå›¾åƒï¼ˆä¸ä¿®æ”¹åŸå›¾ï¼‰
        preview_image = image.copy() if self.enable_preview else None

        # é˜¶æ®µ1: å¤šç­–ç•¥ä¼ ç»Ÿæ£€æµ‹ï¼ˆä¿æŠ¤åŸå›¾è´¨é‡ï¼‰
        candidate_regions = self._traditional_localization(image)
        print(f"ğŸ“ Traditional method located {len(candidate_regions)} candidate regions")

        # ç”Ÿæˆé˜¶æ®µ1é¢„è§ˆ
        if preview_image is not None and candidate_regions:
            self._draw_detection_preview(preview_image, candidate_regions, stage=1,
                                       title="Stage 1: Candidates", color=(255, 255, 0))

        # é˜¶æ®µ2: ç‰¹å¾éªŒè¯å’Œè¿‡æ»¤
        valid_watermarks = self._feature_verification(image, candidate_regions)
        print(f"âœ… Validation passed {len(valid_watermarks)} watermark regions")

        # ç”Ÿæˆé˜¶æ®µ2é¢„è§ˆ
        if preview_image is not None and valid_watermarks:
            valid_regions = [info['bbox'] for info in valid_watermarks]
            self._draw_detection_preview(preview_image, valid_regions, stage=2,
                                       title="Stage 2: Validated", color=(0, 255, 255))

        # é˜¶æ®µ3: ç²¾ç¡®maskç”Ÿæˆ
        detections = self._generate_precise_masks(image, valid_watermarks)
        print(f"ğŸ¯ Generated {len(detections)} precise watermark masks")

        # ç”Ÿæˆæœ€ç»ˆé¢„è§ˆ
        if preview_image is not None and detections:
            final_regions = [det.bbox for det in detections]
            self._draw_detection_preview(preview_image, final_regions, stage=3,
                                       title="Stage 3: Final", color=(0, 255, 0))

            # ä¿å­˜é¢„è§ˆå›¾åƒ
            if preview_path:
                self._save_detection_preview(preview_image, detections, preview_path)

        return detections

    def _traditional_localization(self, image: np.ndarray) -> List[np.ndarray]:
        """
        é˜¶æ®µ1: å¤šç­–ç•¥ä¼ ç»Ÿæ£€æµ‹ - ä¿æŠ¤åŸå›¾è´¨é‡

        æ ¸å¿ƒä¼˜åŒ–ï¼š
        - ä½¿ç”¨è½»é‡çº§è¾¹ç¼˜æ£€æµ‹ï¼Œé¿å…æ¨¡ç³Š
        - å¤šå°ºåº¦å¹¶è¡Œæ£€æµ‹ï¼Œæé«˜å¬å›ç‡
        - æ™ºèƒ½åˆå¹¶ï¼Œå‡å°‘è¯¯æ£€
        - ä¿æŠ¤åŸå›¾åƒç´ ï¼Œä¸è¿›è¡Œç ´åæ€§å¤„ç†
        """
        print("ğŸ” Using multi-strategy traditional detection...")

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        all_candidates = []

        # ç­–ç•¥1: è½»é‡çº§Cannyè¾¹ç¼˜æ£€æµ‹ï¼ˆä¿æŠ¤ç»†èŠ‚ï¼‰
        edges1 = cv2.Canny(gray, 30, 80)  # æ›´ä½çš„é˜ˆå€¼ï¼Œæ£€æµ‹æ›´ç»†çš„è¾¹ç¼˜
        kernel1 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))  # æ›´å°çš„æ ¸
        closed1 = cv2.morphologyEx(edges1, cv2.MORPH_CLOSE, kernel1, iterations=1)
        candidates1 = self._extract_regions_from_mask(closed1, min_area=15)  # æ›´å°çš„æœ€å°é¢ç§¯
        all_candidates.extend(candidates1)

        # ç­–ç•¥2: åŸºäºå¯¹æ¯”åº¦çš„æ£€æµ‹ï¼ˆæ£€æµ‹åŠé€æ˜æ°´å°ï¼‰
        blur = cv2.GaussianBlur(gray, (3, 3), 0)  # è½»å¾®æ¨¡ç³Šä¿æŠ¤ç»†èŠ‚
        contrast = cv2.absdiff(gray, blur)
        _, thresh2 = cv2.threshold(contrast, 8, 255, cv2.THRESH_BINARY)  # æ›´ä½çš„é˜ˆå€¼
        kernel2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
        closed2 = cv2.morphologyEx(thresh2, cv2.MORPH_CLOSE, kernel2, iterations=1)
        candidates2 = self._extract_regions_from_mask(closed2, min_area=12)
        all_candidates.extend(candidates2)

        # ç­–ç•¥3: è‡ªé€‚åº”é˜ˆå€¼æ£€æµ‹ï¼ˆé€‚åº”ä¸åŒäº®åº¦ï¼‰
        adaptive_thresh = cv2.adaptiveThreshold(
            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
        )
        kernel3 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
        closed3 = cv2.morphologyEx(adaptive_thresh, cv2.MORPH_CLOSE, kernel3, iterations=1)
        candidates3 = self._extract_regions_from_mask(closed3, min_area=10)
        all_candidates.extend(candidates3)

        # æ™ºèƒ½å»é‡åˆå¹¶
        all_candidates = self._merge_overlapping_regions(all_candidates, iou_threshold=0.5)

        # è¿‡æ»¤æ˜æ˜¾ä¸æ˜¯æ°´å°çš„åŒºåŸŸï¼ˆåŸºäºå½¢çŠ¶ç‰¹å¾ï¼‰
        filtered_candidates = []
        for region in all_candidates:
            x1, y1, x2, y2 = region
            w, h = x2 - x1, y2 - y1

            # è¿‡æ»¤è¿‡å¤§æˆ–è¿‡å°çš„åŒºåŸŸ (æ”¾å®½é™åˆ¶)
            area = w * h
            if area < 10 or area > image.shape[0] * image.shape[1] * 0.15:  # å…è®¸æ›´å¤§çš„åŒºåŸŸï¼Œé™ä½æœ€å°é¢ç§¯
                continue

            # è¿‡æ»¤å®½é«˜æ¯”å¼‚å¸¸çš„åŒºåŸŸ (æ”¾å®½é™åˆ¶)
            aspect_ratio = max(w, h) / min(w, h) if min(w, h) > 0 else 10
            if aspect_ratio > 30:  # æ”¾å®½å®½é«˜æ¯”é™åˆ¶
                continue

            filtered_candidates.append(region)

        print(f"ğŸ¯ Multi-strategy detection found {len(filtered_candidates)} regions")
        return filtered_candidates

    def _feature_verification(self, image: np.ndarray, candidate_regions: List[np.ndarray]) -> List[Dict]:
        """
        é˜¶æ®µ2: å¤šç‰¹å¾éªŒè¯

        éªŒè¯æ ‡å‡†ï¼š
        1. é‡å¤æ€§ï¼šæ°´å°é€šå¸¸é‡å¤å‡ºç°
        2. é€æ˜åº¦ï¼šæ°´å°é€šå¸¸åŠé€æ˜
        3. ä½ç½®ï¼šæ°´å°é€šå¸¸åœ¨å›¾åƒè¾¹ç¼˜æˆ–è§’è½
        4. ä¸€è‡´æ€§ï¼šç›¸ä¼¼åŒºåŸŸåº”è¯¥æœ‰ç›¸ä¼¼ç‰¹å¾
        """
        verified_regions = []

        for region in candidate_regions:
            features = self._extract_watermark_features(image, region)

            # ç»¼åˆè¯„åˆ†
            score = self._compute_watermark_score(features)

            if score > 0.15:  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼ï¼Œç¡®ä¿ä¸é—æ¼æ°´å°
                verified_regions.append({
                    'bbox': region,
                    'features': features,
                    'score': score
                })

        return verified_regions

    def _generate_precise_masks(self, image: np.ndarray, verified_regions: List[Dict]) -> List[WatermarkDetection]:
        """
        é˜¶æ®µ3: ç”Ÿæˆç²¾ç¡®mask

        æ–¹æ³•ï¼š
        1. åŸºäºAIåˆ†å‰²ç»“æœ
        2. ç»“åˆå±€éƒ¨å¯¹æ¯”åº¦åˆ†æ
        3. å½¢æ€å­¦ä¼˜åŒ–
        """
        detections = []

        for region_info in verified_regions:
            bbox = region_info['bbox']

            # æ‰©å¤§è¾¹ç•Œæ¡†ä»¥åŒ…å«æ ‡ç‚¹ç¬¦å·
            expanded_bbox = self._expand_bbox_for_punctuation(image, bbox)

            # ä½¿ç”¨æ‰©å¤§åçš„è¾¹ç•Œæ¡†ç”Ÿæˆmaskï¼Œç¡®ä¿æ ‡ç‚¹ä¹Ÿè¢«åŒ…å«
            precise_mask = self._refine_mask_with_contrast(image, expanded_bbox)

            # OCRéªŒè¯ï¼ˆå¯é€‰ï¼Œç”¨äºæå–æ–‡å­—å†…å®¹ï¼‰
            text = ""
            confidence = region_info['score']

            if self.reader:
                try:
                    roi = image[bbox[1]:bbox[3], bbox[0]:bbox[2]]
                    results = self.reader.readtext(roi, detail=0)
                    if results:
                        text = results[0]
                except:
                    pass

            detection = WatermarkDetection(
                bbox=bbox,
                confidence=confidence,
                text=text,
                mask=precise_mask,
                category='watermark'
            )

            detections.append(detection)

        return detections

    def _expand_bbox_for_punctuation(self, image: np.ndarray, bbox: np.ndarray) -> np.ndarray:
        """
        æ™ºèƒ½æ‰©å¤§æ–‡å­—è¾¹ç•Œæ¡†ï¼Œç¡®ä¿åŒ…å«æ ‡ç‚¹ç¬¦å·

        ç­–ç•¥ï¼š
        1. å‘å³ä¸‹æ–¹æ‰©å±•ï¼Œè¦†ç›–å¯èƒ½çš„æ ‡ç‚¹ä½ç½®
        2. åŸºäºæ–‡å­—å°ºå¯¸è®¡ç®—åˆé€‚çš„æ‰©å±•èŒƒå›´
        3. é¿å…æ‰©å±•åˆ°å›¾åƒè¾¹ç•Œå¤–
        """
        h, w = image.shape[:2]
        x1, y1, x2, y2 = bbox

        # è®¡ç®—æ–‡å­—å°ºå¯¸
        text_width = x2 - x1
        text_height = y2 - y1

        # æ‰©å±•ç­–ç•¥ï¼š
        # å³è¾¹ï¼šæ‰©å±•æ–‡å­—å®½åº¦çš„30-50%ï¼Œè¦†ç›–å¥å·ã€é€—å·ç­‰
        # ä¸‹è¾¹ï¼šæ‰©å±•æ–‡å­—é«˜åº¦çš„20-40%ï¼Œè¦†ç›–ä¸‹æ ‡ç‚¹
        # å·¦è¾¹ï¼šè½»å¾®æ‰©å±•ï¼Œé¿å…é—æ¼
        # ä¸Šè¾¹ï¼šè½»å¾®æ‰©å±•

        expand_right = int(text_width * 0.4)   # å‘å³æ‰©å±•40%
        expand_bottom = int(text_height * 0.3) # å‘ä¸‹æ‰©å±•30%
        expand_left = int(text_width * 0.1)    # å‘å·¦æ‰©å±•10%
        expand_top = int(text_height * 0.1)    # å‘ä¸Šæ‰©å±•10%

        # åº”ç”¨æ‰©å±•ï¼Œä½†ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
        new_x1 = max(0, x1 - expand_left)
        new_y1 = max(0, y1 - expand_top)
        new_x2 = min(w, x2 + expand_right)
        new_y2 = min(h, y2 + expand_bottom)

        return np.array([new_x1, new_y1, new_x2, new_y2])

    def _extract_watermark_features(self, image: np.ndarray, bbox: np.ndarray) -> Dict:
        """æå–æ°´å°ç‰¹å¾"""
        x1, y1, x2, y2 = bbox
        roi = image[y1:y2, x1:x2]

        features = {}

        # 1. é€æ˜åº¦ç‰¹å¾ï¼ˆæ°´å°é€šå¸¸åŠé€æ˜ï¼‰
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        features['transparency'] = self._compute_transparency_score(gray)

        # 2. é‡å¤æ€§ç‰¹å¾
        features['repetitiveness'] = self._compute_repetition_score(roi)

        # 3. ä½ç½®ç‰¹å¾ï¼ˆæ°´å°å¸¸åœ¨è¾¹ç¼˜ï¼‰
        features['position'] = self._compute_position_score(bbox, image.shape)

        # 4. å¯¹æ¯”åº¦ç‰¹å¾ï¼ˆæ°´å°å¯¹æ¯”åº¦é€‚ä¸­ï¼‰
        features['contrast'] = self._compute_contrast_score(roi)

        return features

    def _compute_watermark_score(self, features: Dict) -> float:
        """è®¡ç®—ç»¼åˆæ°´å°è¯„åˆ†"""
        weights = {
            'transparency': 0.3,
            'repetitiveness': 0.3,
            'position': 0.2,
            'contrast': 0.2
        }

        score = sum(features[key] * weights.get(key, 0) for key in features.keys())
        return min(1.0, max(0.0, score))

    def _compute_transparency_score(self, gray_roi: np.ndarray) -> float:
        """è®¡ç®—é€æ˜åº¦è¯„åˆ†ï¼ˆæ°´å°é€šå¸¸åŠé€æ˜ï¼Œä¸å¤ªæš—ä¹Ÿä¸å¤ªäº®ï¼‰"""
        hist = cv2.calcHist([gray_roi], [0], None, [256], [0, 256])
        hist = hist.flatten() / hist.sum()

        # æ°´å°é€šå¸¸åœ¨ä¸­é—´ç°åº¦èŒƒå›´
        mid_range = hist[64:192].sum()
        return float(mid_range)

    def _compute_repetition_score(self, roi: np.ndarray) -> float:
        """è®¡ç®—é‡å¤æ€§è¯„åˆ†"""
        # ç®€åŒ–çš„é‡å¤æ€§æ£€æµ‹ï¼šåŸºäºFFTé¢‘åŸŸåˆ†æ
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)

        # è®¡ç®—FFT
        f = np.fft.fft2(gray)
        fshift = np.fft.fftshift(f)
        magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1)

        # é‡å¤å›¾æ¡ˆé€šå¸¸åœ¨é¢‘åŸŸæœ‰æ˜æ˜¾å³°å€¼
        # è¿™é‡Œç®€åŒ–ä¸ºè®¡ç®—é¢‘åŸŸèƒ½é‡åˆ†å¸ƒçš„å‡åŒ€æ€§
        hist, _ = np.histogram(magnitude_spectrum.flatten(), bins=50)
        hist = hist / hist.sum()

        # å‡åŒ€åˆ†å¸ƒè¯´æ˜æœ‰é‡å¤å›¾æ¡ˆ
        uniformity = 1.0 - np.std(hist)
        return float(uniformity)

    def _compute_position_score(self, bbox: np.ndarray, image_shape: Tuple) -> float:
        """è®¡ç®—ä½ç½®è¯„åˆ†ï¼ˆæ°´å°å¸¸åœ¨è¾¹ç¼˜ï¼‰"""
        h, w = image_shape[:2]
        x1, y1, x2, y2 = bbox

        center_x = (x1 + x2) / 2
        center_y = (y1 + y2) / 2

        # è®¡ç®—åˆ°å›¾åƒä¸­å¿ƒçš„è·ç¦»
        dist_to_center = np.sqrt(((center_x - w/2) / (w/2)) ** 2 +
                                ((center_y - h/2) / (h/2)) ** 2)

        # æ°´å°é€šå¸¸ä¸åœ¨å›¾åƒä¸­å¿ƒï¼ˆè·ç¦»ä¸­å¿ƒè¶Šè¿œè¶Šå¯èƒ½æ˜¯æ°´å°ï¼‰
        return min(1.0, dist_to_center)

    def _compute_contrast_score(self, roi: np.ndarray) -> float:
        """è®¡ç®—å¯¹æ¯”åº¦è¯„åˆ†ï¼ˆæ°´å°å¯¹æ¯”åº¦é€‚ä¸­ï¼‰"""
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)

        # è®¡ç®—å±€éƒ¨å¯¹æ¯”åº¦
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        dilated = cv2.dilate(gray, kernel)
        eroded = cv2.erode(gray, kernel)

        contrast = cv2.absdiff(dilated, eroded)
        mean_contrast = np.mean(contrast)

        # æ°´å°å¯¹æ¯”åº¦é€šå¸¸é€‚ä¸­ï¼ˆä¸é«˜ä¸ä½ï¼‰
        normalized_contrast = min(1.0, mean_contrast / 50.0)
        return normalized_contrast

    def _refine_mask_with_contrast(self, image: np.ndarray, bbox: np.ndarray) -> np.ndarray:
        """åŸºäºå¤šå°ºåº¦å¯¹æ¯”åº¦åˆ†æç²¾ç¡®åŒ–mask - ä¼˜åŒ–ç‰ˆï¼Œç¡®ä¿æ–‡å­—å’Œæ ‡ç‚¹éƒ½è¢«æ£€æµ‹"""
        x1, y1, x2, y2 = bbox
        roi = image[y1:y2, x1:x2]

        # é¢„å¤„ç†ï¼šä¿æŒç»†èŠ‚çš„åŒè¾¹æ»¤æ³¢
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        # ä½¿ç”¨è¾ƒä¿å®ˆçš„åŒè¾¹æ»¤æ³¢å‚æ•°ï¼Œä¿æŒæ›´å¤šç»†èŠ‚
        bilateral = cv2.bilateralFilter(gray, d=5, sigmaColor=50, sigmaSpace=50)

        # ä¼˜åŒ–çš„å¯¹æ¯”åº¦æ£€æµ‹ - ç¡®ä¿æ–‡å­—å’Œæ ‡ç‚¹éƒ½è¢«æ£€æµ‹
        # ä½¿ç”¨ä¸¤ç§å°ºåº¦çš„å¯¹æ¯”åº¦æ£€æµ‹ï¼šä¸»è¦æ–‡å­—å’Œç»†èŠ‚æ ‡ç‚¹

        # ä¸»è¦æ–‡å­—æ£€æµ‹ï¼ˆä¸­ç­‰å°ºåº¦ï¼‰
        blur_main = cv2.GaussianBlur(bilateral, (3, 3), 0)
        contrast_main = cv2.absdiff(bilateral, blur_main)

        # æ ‡ç‚¹ç»†èŠ‚æ£€æµ‹ï¼ˆå°å°ºåº¦ï¼Œæ›´æ•æ„Ÿï¼‰
        blur_detail = cv2.GaussianBlur(bilateral, (1, 1), 0)
        contrast_detail = cv2.absdiff(bilateral, blur_detail)

        # ç»“åˆä¸¤ç§å¯¹æ¯”åº¦ï¼šä¸»è¦æ–‡å­— + æ ‡ç‚¹ç»†èŠ‚
        combined_contrast = cv2.addWeighted(contrast_main, 0.7, contrast_detail, 0.3, 0)

        # ä¼˜åŒ–çš„äºŒå€¼åŒ–ç­–ç•¥ - ç¡®ä¿æ–‡å­—å’Œæ ‡ç‚¹éƒ½è¢«æ­£ç¡®åˆ†å‰²
        # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ + OTSUï¼Œç¡®ä¿ç»†èŠ‚è¢«ä¿ç•™
        thresh_adaptive = cv2.adaptiveThreshold(
            combined_contrast, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 7, 2
        )

        # OTSUé˜ˆå€¼ä½œä¸ºè¡¥å……ï¼Œç¡®ä¿å¼±å¯¹æ¯”åº¦åŒºåŸŸä¹Ÿè¢«æ£€æµ‹
        _, thresh_otsu = cv2.threshold(combined_contrast, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        # åˆå¹¶ä¸¤ç§é˜ˆå€¼ç»“æœï¼Œç¡®ä¿æ–‡å­—å’Œæ ‡ç‚¹éƒ½è¢«è¦†ç›–
        combined_thresh = cv2.bitwise_or(thresh_adaptive, thresh_otsu)

        # ä¼˜åŒ–çš„å½¢æ€å­¦å¤„ç† - è¿æ¥æ–‡å­—å’Œæ ‡ç‚¹ï¼Œç¡®ä¿å®Œæ•´æ€§
        # æ­¥éª¤1: ä¸­ç­‰é—­è¿ç®—è¿æ¥æ–­å¼€çš„ç¬”ç”»å’Œæ ‡ç‚¹
        kernel_medium = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        refined = cv2.morphologyEx(combined_thresh, cv2.MORPH_CLOSE, kernel_medium, iterations=2)

        # æ­¥éª¤2: å°å¹…è†¨èƒ€ç¡®ä¿è¦†ç›–æ‰€æœ‰æ°´å°ç»†èŠ‚
        kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
        refined = cv2.dilate(refined, kernel_small, iterations=1)

        # æ­¥éª¤3: æœ€ç»ˆé—­è¿ç®—ç¡®ä¿æ ‡ç‚¹ä¸æ–‡å­—å®Œå…¨è¿æ¥
        refined = cv2.morphologyEx(refined, cv2.MORPH_CLOSE, kernel_medium, iterations=1)

        # æ­¥éª¤4: Cannyè¾¹ç¼˜è¡¥å…… - æ£€æµ‹å¯èƒ½é—æ¼çš„æ ‡ç‚¹è¾¹ç¼˜
        canny = cv2.Canny(bilateral, 20, 60)  # é€‚ä¸­çš„é˜ˆå€¼ï¼Œé¿å…å™ªå£°
        # åªåœ¨å¯¹æ¯”åº¦åŒºåŸŸæ·»åŠ Cannyè¾¹ç¼˜
        _, contrast_mask = cv2.threshold(combined_contrast, 10, 255, cv2.THRESH_BINARY)
        canny_filtered = cv2.bitwise_and(canny, contrast_mask)
        refined = cv2.bitwise_or(refined, canny_filtered)

        # æ­¥éª¤5: è½»å¾®æ¸…ç† - å»é™¤å­¤ç«‹å™ªå£°ç‚¹
        kernel_clean = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 1))
        refined = cv2.morphologyEx(refined, cv2.MORPH_OPEN, kernel_clean, iterations=1)

        # åˆ›å»ºå…¨å°ºå¯¸mask
        full_mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
        full_mask[y1:y2, x1:x2] = refined

        return full_mask

    def _extract_regions_from_mask(self, mask: np.ndarray, min_area: int = 50) -> List[np.ndarray]:
        """ä»maskæå–åŒºåŸŸbbox"""
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        regions = []
        for cnt in contours:
            area = cv2.contourArea(cnt)
            if area >= min_area:
                x, y, w, h = cv2.boundingRect(cnt)
                regions.append(np.array([x, y, x + w, y + h]))

        return regions

    def _merge_overlapping_regions(self, regions: List[np.ndarray], iou_threshold: float = 0.3) -> List[np.ndarray]:
        """åˆå¹¶é‡å çš„å€™é€‰åŒºåŸŸï¼Œå‡å°‘é‡å¤"""
        if not regions:
            return regions

        merged = []
        used = [False] * len(regions)

        for i, region1 in enumerate(regions):
            if used[i]:
                continue

            x1_1, y1_1, x2_1, y2_1 = region1
            merged_region = region1.copy()

            for j, region2 in enumerate(regions):
                if i == j or used[j]:
                    continue

                x1_2, y1_2, x2_2, y2_2 = region2

                # è®¡ç®—äº¤å¹¶æ¯” (IoU)
                x1_i = max(x1_1, x1_2)
                y1_i = max(y1_1, y1_2)
                x2_i = min(x2_1, x2_2)
                y2_i = min(y2_1, y2_2)

                if x2_i > x1_i and y2_i > y1_i:
                    intersection = (x2_i - x1_i) * (y2_i - y1_i)
                    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
                    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
                    union = area1 + area2 - intersection
                    iou = intersection / union if union > 0 else 0

                    if iou > iou_threshold:
                        # åˆå¹¶åŒºåŸŸ
                        merged_region[0] = min(merged_region[0], region2[0])  # x1
                        merged_region[1] = min(merged_region[1], region2[1])  # y1
                        merged_region[2] = max(merged_region[2], region2[2])  # x2
                        merged_region[3] = max(merged_region[3], region2[3])  # y2
                        used[j] = True

            merged.append(merged_region)

        return merged


    def _draw_detection_preview(self, image: np.ndarray, regions: List[np.ndarray],
                               stage: int, title: str, color: Tuple[int, int, int]):
        """åœ¨é¢„è§ˆå›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ"""
        if not self.enable_preview:
            return

        # ç»˜åˆ¶è¾¹æ¡†
        for region in regions:
            if len(region) == 4:  # [x1, y1, x2, y2] æ ¼å¼
                x1, y1, x2, y2 = region
            else:  # å¤šè¾¹å½¢æ ¼å¼
                x1, y1 = region.min(axis=0)
                x2, y2 = region.max(axis=0)

            # ç»˜åˆ¶çŸ©å½¢è¾¹æ¡†
            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)

            # æ·»åŠ åŒºåŸŸç¼–å·
            cv2.putText(image, str(len(regions)), (int(x1), int(y1) - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

        # æ·»åŠ é˜¶æ®µæ ‡é¢˜ (è‹±æ–‡é¿å…ä¹±ç )
        h, w = image.shape[:2]
        overlay = image.copy()
        cv2.rectangle(overlay, (10, 10), (350, 50), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, image, 0.3, 0, image)

        cv2.putText(image, f"{title} ({len(regions)} regions)", (20, 35),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

    def _save_detection_preview(self, image: np.ndarray, detections: List[WatermarkDetection],
                               output_path: str):
        """ä¿å­˜æ£€æµ‹é¢„è§ˆå›¾åƒ"""
        if not self.enable_preview:
            return

        # åˆ›å»ºæœ€ç»ˆé¢„è§ˆ
        final_preview = image.copy()

        # åœ¨å³ä¸Šè§’æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        h, w = final_preview.shape[:2]
        stats_overlay = final_preview.copy()
        cv2.rectangle(stats_overlay, (w - 350, 10), (w - 10, 120), (0, 0, 0), -1)
        cv2.addWeighted(stats_overlay, 0.7, final_preview, 0.3, 0, final_preview)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_area = sum((det.bbox[2] - det.bbox[0]) * (det.bbox[3] - det.bbox[1]) for det in detections)
        coverage = total_area / (h * w) * 100

        # æ·»åŠ ç»Ÿè®¡æ–‡æœ¬ (è‹±æ–‡é¿å…ä¹±ç )
        cv2.putText(final_preview, f"Detection Stats", (w - 340, 35),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(final_preview, f"Regions: {len(detections)}", (w - 340, 55),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        cv2.putText(final_preview, f"Coverage: {coverage:.1f}%", (w - 340, 75),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        cv2.putText(final_preview, f"Method: Traditional", (w - 340, 95),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)

        # ä¿å­˜åˆ°å¯¹åº”çš„è½®æ¬¡ç›®å½•
        import os
        preview_dir = os.path.dirname(output_path)
        os.makedirs(preview_dir, exist_ok=True)

        preview_name = os.path.basename(output_path).replace('.png', '_detection_preview.jpg')
        preview_path = os.path.join(preview_dir, preview_name)

        cv2.imwrite(preview_path, final_preview)
        print(f"ğŸ“¸ Detection preview saved: {preview_path}")

    def generate_mask(self, image: np.ndarray, preview_path: Optional[str] = None) -> np.ndarray:
        """
        ç”Ÿæˆæœ€ç»ˆæ°´å°mask - ç®€åŒ–çš„ä¸»æ¥å£

        è¿”å›: äºŒå€¼mask (255=æ°´å°åŒºåŸŸ, 0=èƒŒæ™¯)
        """
        detections = self.detect_watermarks(image, preview_path)

        # åˆå¹¶æ‰€æœ‰æ£€æµ‹ç»“æœ
        final_mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)

        for detection in detections:
            final_mask = cv2.bitwise_or(final_mask, detection.mask)

        # æœ€ç»ˆå½¢æ€å­¦ä¼˜åŒ–
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, kernel, iterations=1)
        final_mask = cv2.dilate(final_mask, kernel, iterations=1)

        coverage = np.count_nonzero(final_mask) / final_mask.size * 100
        print(f"ğŸ’¾ Mask coverage: {coverage:.1f}%")
        return final_mask


def main():
    """ä½¿ç”¨ç¤ºä¾‹ - æ”¯æŒè½®æ¬¡ç›®å½•ç»“æ„"""
    import argparse
    import os

    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆæ°´å°æ£€æµ‹å™¨')
    parser.add_argument('-r', '--round', required=True, help='è½®æ¬¡ç›®å½• (å¦‚: 1, 2, 3)')
    parser.add_argument('--preview', action='store_true', help='ç”Ÿæˆæ£€æµ‹è¿‡ç¨‹é¢„è§ˆå›¾')
    parser.add_argument('--simple-preview', action='store_true', help='ç”Ÿæˆç®€å•çš„æœ€ç»ˆç»“æœé¢„è§ˆå›¾')
    parser.add_argument('--no-preview', action='store_true', help='ç¦ç”¨æ‰€æœ‰é¢„è§ˆåŠŸèƒ½')

    args = parser.parse_args()

    # æ„å»ºè·¯å¾„
    round_dir = args.round
    input_path = 'sample.jpg'
    output_path = os.path.join(round_dir, 'mask.png')

    # ç¡®ä¿è½®æ¬¡ç›®å½•å­˜åœ¨
    os.makedirs(round_dir, exist_ok=True)

    # åŠ è½½å›¾åƒ
    image = cv2.imread(input_path)
    if image is None:
        print(f"âŒ Failed to load image: {input_path}")
        return

    print(f"ğŸ¯ Processing round {args.round}: {input_path}")

    # åˆ›å»ºæ£€æµ‹å™¨
    enable_preview = not args.no_preview
    detector = OptimizedWatermarkDetector(enable_preview=enable_preview)

    # ç¡®å®šé¢„è§ˆè·¯å¾„
    preview_path = None
    if args.preview or args.simple_preview:
        preview_path = os.path.join(round_dir, 'detection_preview.jpg')

    # ç”Ÿæˆmask
    mask = detector.generate_mask(image, preview_path)

    # ä¿å­˜ç»“æœ
    cv2.imwrite(output_path, mask)
    print(f"ğŸ’¾ Mask saved: {output_path}")

    # ç”Ÿæˆç®€å•é¢„è§ˆï¼ˆå¦‚æœéœ€è¦ä¸”è¿˜æ²¡æœ‰ç”Ÿæˆæ£€æµ‹é¢„è§ˆï¼‰
    if args.simple_preview and not args.preview:
        simple_preview_path = os.path.join(round_dir, 'simple_preview.jpg')
        overlay = image.copy()
        overlay[mask > 127] = [0, 0, 255]  # çº¢è‰²æ ‡è®°æ°´å°åŒºåŸŸ

        # æ·»åŠ ç®€å•çš„ç»Ÿè®¡ä¿¡æ¯
        h, w = overlay.shape[:2]
        coverage = cv2.countNonZero(mask) / (h * w) * 100
        cv2.putText(overlay, f"Coverage: {coverage:.1f}%", (20, 40),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
        cv2.putText(overlay, f"Round: {args.round}", (20, 70),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        cv2.imwrite(simple_preview_path, overlay)
        print(f"ğŸ–¼ï¸ Simple preview saved: {simple_preview_path}")

    # è‡ªåŠ¨è¿è¡Œæ°´å°å»é™¤
    print("ğŸ§¹ Starting automatic watermark removal...")
    run_watermark_removal(round_dir, input_path, output_path)

    print(f"âœ… Round {args.round} completed!")

def run_watermark_removal(round_dir: str, input_image: str, mask_file: str):
    """è¿è¡Œæ°´å°å»é™¤å‘½ä»¤"""
    import subprocess
    import os

    # é¦–å…ˆæ£€æŸ¥iopaintæ˜¯å¦å¯ç”¨
    try:
        result = subprocess.run(["iopaint", "--help"], capture_output=True, text=True, timeout=10)
        iopaint_available = result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        iopaint_available = False

    if not iopaint_available:
        print("âš ï¸ iopaint command not found. Skipping automatic watermark removal.")
        print("ğŸ’¡ To enable automatic removal, install IOPaint and ensure it's in PATH")
        print("   Manual command format:")
        print(f"   iopaint run --model=lama --device=cpu --image={input_image} --mask={mask_file} --output={round_dir}/output.jpg")
        return

    # æ„å»ºè¾“å‡ºè·¯å¾„ï¼ˆåœ¨åŒä¸€ç›®å½•ä¸‹ï¼‰
    base_name = os.path.splitext(os.path.basename(input_image))[0]
    output_file = round_dir

    # æ„å»ºiopaintå‘½ä»¤
    cmd = [
        "iopaint", "run",
        "--model=lama",
        "--device=cpu",
        f"--image={input_image}",
        f"--mask={mask_file}",
        f"--output={output_file}"
    ]

    print(f"ğŸ”§ Running: {' '.join(cmd)}")

    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡é¿å…åº“å†²çª
        env = os.environ.copy()
        env['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(cmd, env=env, capture_output=True, text=True, timeout=300)

        if result.returncode == 0:
            print(f"âœ¨ Watermark removal completed: {output_file}")

            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(output_file):
                file_size = os.path.getsize(output_file)
                print(f"ğŸ“Š Output file size: {file_size} bytes")
            else:
                print("âš ï¸ Output file was not created")

        else:
            print(f"âŒ Watermark removal failed (exit code: {result.returncode})")
            if result.stderr:
                print(f"Error details: {result.stderr[:500]}...")
            print("ğŸ’¡ Manual command:")
            print(f"   export KMP_DUPLICATE_LIB_OK=TRUE")
            print(f"   {' '.join(cmd)}")

    except subprocess.TimeoutExpired:
        print("â° Watermark removal timed out (5 minutes)")
        print("ğŸ’¡ Try running manually with shorter timeout or different model")
    except Exception as e:
        print(f"âŒ Unexpected error during watermark removal: {str(e)}")
        print("ğŸ’¡ Manual command:")
        print(f"   export KMP_DUPLICATE_LIB_OK=TRUE")
        print(f"   {' '.join(cmd)}")

if __name__ == "__main__":
    main()
